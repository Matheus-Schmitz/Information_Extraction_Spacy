{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right spacy version for my code\n",
    "!pip install --force-reinstall spacy==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exclusive-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matheus\\Anaconda3\\lib\\site-packages\\spacy\\util.py:715: UserWarning: [W094] Model 'en_core_web_sm' (2.2.5) specifies an under-constrained spaCy version requirement: >=2.2.2. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.1,<3.1.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "veterinary-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs \n",
    "# python task_2_2.py Matheus_Schmitz_hw02_bios.csv hw2_task2_2.jl 1\n",
    "#input_csv = sys.argv[1]\n",
    "#output_jl = sys.argv[2]\n",
    "#extractor_number = sys.argv[3]\n",
    "\n",
    "input_csv = 'Matheus_Schmitz_hw02_bios.csv'\n",
    "output_jl = 'hw2_task2_2.jl'\n",
    "extractor_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "removed-poison",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from C:\\Users\\Matheus\\Anaconda3\\lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.5\\config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e3796228f4af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\en_core_web_sm\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_config\u001b[1;34m(path, overrides, interpolate)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE053\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m         return config.from_disk(\n\u001b[0;32m    547\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E053] Could not read config.cfg from C:\\Users\\Matheus\\Anaconda3\\lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.5\\config.cfg"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace_lexical = [\n",
    "    {'LOWER': 'born'},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': 'in'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_lexical = [\n",
    "    {'TEXT': {'REGEX': '^(attend|attended|went|studied|University|College|trained|graduated)$'}},\n",
    "    {'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_lexical = [\n",
    "    {'LOWER': {'REGEX': '^(born|son|daughter|parents)$'}},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(to|of|are)$'}},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'LOWER': 'and','OP': '?'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_lexical = [\n",
    "    {'LOWER': {'REGEX': '^(winner|recipient|won|awarded|award-winning|nominated|nominee|nomination)$'}},\n",
    "    {'LOWER': {'REGEX': '^(of|the|on|for|as)$'}},\n",
    "    {'OP': '+'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_lexical = [\n",
    "    {'LOWER': {'REGEX': '^(star|starred|known|appeared|appearing|appearances|debute|album|recorded|play|played|role)$'}},\n",
    "    {'LOWER': {'REGEX': '^(in|for|on|for|as)$'}},\n",
    "    {'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "colleagues_lexical = [\n",
    "    {'IS_TITLE': '+', 'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(starring|collaborating|working|worked|partnering|sharing.*screen.*space|appears.*opposite|opposite)$'}},\n",
    "    {'LOWER': {'REGEX': '^(with|alongside|to)$'}}, {'OP': '*'},\n",
    "    {'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace_syntactic = [\n",
    "    {'POS': 'VERB', 'ORTH': 'born'},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': 'in', 'POS': 'ADP'},\n",
    "    {'ENT_TYPE': 'GPE', 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'ENT_TYPE': 'GPE', 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_syntactic = [\n",
    "    {'TEXT': {'REGEX': '^(attend|attended|went|studied|University|College|trained|graduated)$'}},\n",
    "    {'ENT_TYPE': 'ORG', 'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_syntactic = [\n",
    "    {'LOWER': {'REGEX': '^(born|son|daughter|parents)$'}},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(to|of|are)$'}},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'ENT_TYPE': 'PERSON', 'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'LOWER': 'and','OP': '?'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'ENT_TYPE': 'PERSON', 'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_syntactic = [\n",
    "    {'OP': '*'},\n",
    "    {'POS': 'NOUN', 'IS_TITLE': '+', 'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(winner|recipient|won|awarded|award-winning|nominated|nominee|nomination)$'}},\n",
    "    {'POS': 'ADP', 'TEXT': {'REGEX': '^(of|the|for)$'}, 'OP': '*'},\n",
    "    {'POS': 'NOUN', 'IS_TITLE': '+', 'OP': '*'},\n",
    "    {'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_syntactic = [\n",
    "    {'LOWER': {'REGEX': '^(star|starred|known|appeared|appearing|appearances|debute|album|recorded|play|played|role)$'}},\n",
    "    {'POS': 'ADP', 'TEXT': {'REGEX': '^(in|of|the|for|as)$'}, 'OP': '+'},\n",
    "    {'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "colleagues_syntactic = [\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(starring|collaborating|working|worked|partnering|sharing.*screen.*space|appears.*opposite|opposite|joined)$'}},\n",
    "    {'LOWER': {'REGEX': '^(with|alongside|to)$'}}, {'OP': '*'},\n",
    "    {'ENT_TYPE': 'PERSON'},\n",
    "    {'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to match and return results\n",
    "def spacy_matcher(doc, pattern):\n",
    "    output = []\n",
    "    \n",
    "    # Instantiate the spaCy Matcher\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", None, pattern)\n",
    "    \n",
    "    # Iterate throgh sentences\n",
    "    for sent in doc.sents:\n",
    "\n",
    "        # Match sentences to the pattern\n",
    "        matches = matcher(nlp(sent.text)) \n",
    "        \n",
    "        # If any matches were found, extract them and append to the list of matches for that pair of (document x pattern)\n",
    "        if len(matches)>0:\n",
    "            span = sent[matches[-1][1]:matches[-1][2]] \n",
    "            output.append(span.text)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthplace_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = ' '.join(list_with_text)        \n",
    "        list_with_text = joined.rsplit('as')\n",
    "        return [[' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", list_with_text[i])) \n",
    "                 for i in range(len(list_with_text))][0]\n",
    "                .strip(r'^January$|^February$|^March$|^April$|^May$|^June$|^July$|^August$|^September$|^October$|^November$|^December$|^USA$')][0]\n",
    "    else:\n",
    "        return ''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def education_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = ','.join(list_with_text)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(',| and ', joined)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Clear additional frequent nuisances\n",
    "        output1 = [re.sub(r'^She$|^He$|^January$|^February$|^March$|^April$|^May$|^June$|^July$|^August$|^September$|^October$|^November$|^December$', '', titlecases[i]) for i in range(len(titlecases))]\n",
    "        # Remove any empty strings generated\n",
    "        output2 = [x for x in output1 if x.strip()]\n",
    "        return output2\n",
    "    else:\n",
    "        return list_with_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parents_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = ','.join(list_with_text)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(',| and ', joined)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Clear additional frequent nuisances\n",
    "        output1 = [re.sub(r'^She$|^He$|^January$|^February$|^March$|^April$|^May$|^June$|^July$|^August$|^September$|^October$|^November$|^December$', '', titlecases[i]) for i in range(len(titlecases))]\n",
    "        # Remove any empty strings generated\n",
    "        output2 = [x for x in output1 if x.strip()]\n",
    "        return output2\n",
    "    else:\n",
    "        return list_with_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def awards_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = ','.join(list_with_text)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(',| and | for ', joined)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Remove any empty strings generated\n",
    "        output1 = [x for x in titlecases if x.strip()]\n",
    "        return output1\n",
    "    else:\n",
    "        return list_with_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = ','.join(list_with_text)\n",
    "        # Remove \"known for\"\n",
    "        clean_1 = re.sub('known for', '', joined)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(',| and ', clean_1)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Remove any empty strings generated\n",
    "        output1 = [x for x in titlecases if x.strip()]\n",
    "        return output1\n",
    "    else:\n",
    "        return list_with_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colleagues_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Remove double quotations \" around some movie names\n",
    "        unquoted = [x.strip('\"') for x in list_with_text]\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(',| and | in', unquoted[0])\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Remove any empty strings generated\n",
    "        output1 = [x for x in titlecases if x.strip()]\n",
    "        return output1\n",
    "    else:\n",
    "        return list_with_text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical\n",
    "if extractor_number == 0:\n",
    "    # Open the jsonlines to be written\n",
    "    with open(output_jl, 'w') as hw2_lexical:\n",
    "        # Iterate through the csv with bios\n",
    "        for url, bio in tqdm(csv.reader(open(input_csv, encoding='utf-8')),\n",
    "                             total=sum(1 for row in csv.reader(open(input_csv, encoding='utf-8')))):\n",
    "\n",
    "            # Create dict to store the outputs of each row\n",
    "            output = {}\n",
    "\n",
    "            # Populate the outputs with spaCy's Matcher\n",
    "            output['url'] = url\n",
    "            output['birthplace'] = spacy_matcher(nlp(bio), birthplace_lexical)\n",
    "            output['education'] = spacy_matcher(nlp(bio), education_lexical)\n",
    "            output['parents'] = spacy_matcher(nlp(bio), parents_lexical)\n",
    "            output['awards'] = spacy_matcher(nlp(bio), awards_lexical)        \n",
    "            output['performances'] = spacy_matcher(nlp(bio), performances_lexical)\n",
    "            output['colleagues'] = spacy_matcher(nlp(bio), colleagues_lexical)\n",
    "            \n",
    "            # Keep only Title Case words\n",
    "            output['birthplace'] = birthplace_filter(output['birthplace'])\n",
    "            output['education'] = education_filter(output['education'])\n",
    "            output['parents'] = parents_filter(output['parents'])\n",
    "            output['awards'] = awards_filter(output['awards'])\n",
    "            output['performances'] = performances_filter(output['performances'])\n",
    "            output['colleagues'] = colleagues_filter(output['colleagues']) \n",
    "\n",
    "            # Write a csv row as a line in jsonlines\n",
    "            hw2_lexical.write(str(output)+'\\n')\n",
    "\n",
    "        # Close the jsonlines file\n",
    "        hw2_lexical.close()\n",
    "\n",
    "# Syntactic       \n",
    "elif extractor_number == 1:\n",
    "    # Open the jsonlines to be written\n",
    "    with open(output_jl, 'w') as hw2_syntactic:\n",
    "        # Iterate through the csv with bios   \n",
    "        for url, bio in tqdm(csv.reader(open(input_csv, encoding='utf-8')),\n",
    "                             total=sum(1 for row in csv.reader(open(input_csv, encoding='utf-8')))):\n",
    "\n",
    "            # Create dict to store the outputs of each row\n",
    "            output = {}\n",
    "\n",
    "            # Populate the outputs with spaCy's Matcher\n",
    "            output['url'] = url\n",
    "            output['birthplace'] = spacy_matcher(nlp(bio), birthplace_syntactic)\n",
    "            output['education'] = spacy_matcher(nlp(bio), education_syntactic)\n",
    "            output['parents'] = spacy_matcher(nlp(bio), parents_syntactic)\n",
    "            output['awards'] = spacy_matcher(nlp(bio), awards_syntactic)        \n",
    "            output['performances'] = spacy_matcher(nlp(bio), performances_syntactic)\n",
    "            output['colleagues'] = spacy_matcher(nlp(bio), colleagues_syntactic)\n",
    "                                        \n",
    "            # Keep only nouns for certain outputs\n",
    "            output['birthplace'] = birthplace_filter(output['birthplace'])\n",
    "            output['education'] = education_filter(output['education'])\n",
    "            output['parents'] = parents_filter(output['parents'])\n",
    "            output['awards'] = awards_filter(output['awards'])\n",
    "            output['performances'] = performances_filter(output['performances'])\n",
    "            output['colleagues'] = colleagues_filter(output['colleagues'])\n",
    "                                        \n",
    "            # Write a csv row as a line in jsonlines\n",
    "            hw2_syntactic.write(str(output)+'\\n')\n",
    "\n",
    "        # Close the jsonlines file\n",
    "        hw2_syntactic.close()\n",
    "\n",
    "# Error    \n",
    "else:\n",
    "    raise ValueError('Input either 0 for lexical patterns or 1 for syntactic patterns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-george",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
