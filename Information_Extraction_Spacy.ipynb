{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "characteristic-stress",
   "metadata": {},
   "source": [
    "# Information Extraction with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-conditioning",
   "metadata": {},
   "source": [
    "Matheus Schmitz\n",
    "<br><a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a></br>\n",
    "<br><a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-sustainability",
   "metadata": {},
   "source": [
    "## Install spaCy 2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moving-chocolate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing spaCy 2.2.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print('Installing spaCy 2.2.4')\n",
    "os.system('pip install -q -q -q --force-reinstall spacy==2.2.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-american",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "electoral-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-footage",
   "metadata": {},
   "source": [
    "## Visualize Web Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bottom-chamber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped_data.shape: (982, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.imdb.com/name/nm2604107/bio</td>\n",
       "      <td>Wolf-Guido Grasenick was born on January 30, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.imdb.com/name/nm0001873/bio</td>\n",
       "      <td>Steven Zaillian was born on January 30, 1953 i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.imdb.com/name/nm12027010/bio</td>\n",
       "      <td>Michal Kasprzak was born on January 30, 1985 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.imdb.com/name/nm11017764/bio</td>\n",
       "      <td>Stefan Gemmel was born on January 30, 1970 in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.imdb.com/name/nm0014294/bio</td>\n",
       "      <td>Barbara M. Ahren was born on January 30, 1950 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0  \\\n",
       "0   https://www.imdb.com/name/nm2604107/bio   \n",
       "1   https://www.imdb.com/name/nm0001873/bio   \n",
       "2  https://www.imdb.com/name/nm12027010/bio   \n",
       "3  https://www.imdb.com/name/nm11017764/bio   \n",
       "4   https://www.imdb.com/name/nm0014294/bio   \n",
       "\n",
       "                                                   1  \n",
       "0  Wolf-Guido Grasenick was born on January 30, 1...  \n",
       "1  Steven Zaillian was born on January 30, 1953 i...  \n",
       "2  Michal Kasprzak was born on January 30, 1985 a...  \n",
       "3  Stefan Gemmel was born on January 30, 1970 in ...  \n",
       "4  Barbara M. Ahren was born on January 30, 1950 ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data = pd.read_csv(\"Matheus_Schmitz_hw02_bios.csv\", header=None)\n",
    "print(f'scraped_data.shape: {scraped_data.shape}')\n",
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-amsterdam",
   "metadata": {},
   "source": [
    "**Check a Sample Text From Which to Extract Information**\n",
    "\n",
    "This data was crawled from the author biography in IMDB, and thus has a lot of relevant information, but all of it is buried in the middle of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "former-romania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Devon Greenwood recently made is film debut in Cry Myself Awake, a short film written and directed by Ryan Allen. Prior to film, he appeared in multiple stage plays/musicals. Devon also performed in Peter Pan at the iTheatrics Junior Theater Festival in Atlanta, GA along with his elementary school drama team.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data.sample(1).values[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-animal",
   "metadata": {},
   "source": [
    "## Load spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "official-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-spell",
   "metadata": {},
   "source": [
    "## Design Extration Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-nicaragua",
   "metadata": {},
   "source": [
    "**Birthplace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "arbitrary-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace_lexical = [\n",
    "    {'LOWER': 'born'},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': 'in'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "continent-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthplace_syntactic = [\n",
    "    {'POS': 'VERB', 'ORTH': 'born'},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': 'in', 'POS': 'ADP'},\n",
    "    {'ENT_TYPE': 'GPE', 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'ENT_TYPE': 'GPE', 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-communication",
   "metadata": {},
   "source": [
    "**Education**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "hollow-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_lexical = [\n",
    "    {'TEXT': {'REGEX': '^(attend|attended|went|studied|University|College|trained|graduated)$'}},\n",
    "    {'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "boolean-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_syntactic = [\n",
    "    {'TEXT': {'REGEX': '^(attend|attended|went|studied|University|College|trained|graduated)$'}},\n",
    "    {'ENT_TYPE': 'ORG', 'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-shuttle",
   "metadata": {},
   "source": [
    "**Parents**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "unusual-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_lexical = [\n",
    "    {'LOWER': {'REGEX': '^(born|son|daughter|parents)$'}},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(to|of|are)$'}},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'LOWER': 'and','OP': '?'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "brave-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_syntactic = [\n",
    "    {'LOWER': {'REGEX': '^(born|son|daughter|parents)$'}},\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(to|of|are)$'}},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'ENT_TYPE': 'PERSON', 'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'LOWER': 'and','OP': '?'},\n",
    "    {'TEXT': {'REGEX': '\\s*'}, 'OP': '*'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    {'ENT_TYPE': 'PERSON', 'TEXT': {'REGEX': '\\s*'}, 'OP': '+'},\n",
    "    {'IS_PUNCT': True, 'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-ultimate",
   "metadata": {},
   "source": [
    "**Awards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "previous-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_lexical = [\n",
    "    {'LOWER': {'REGEX': '^(winner|recipient|won|awarded|award-winning|nominated|nominee|nomination)$'}},\n",
    "    {'LOWER': {'REGEX': '^(of|the|on|for|as)$'}},\n",
    "    {'OP': '+'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "banned-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_syntactic = [\n",
    "    {'OP': '*'},\n",
    "    {'POS': 'NOUN', 'IS_TITLE': '+', 'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(winner|recipient|won|awarded|award-winning|nominated|nominee|nomination)$'}},\n",
    "    {'POS': 'ADP', 'TEXT': {'REGEX': '^(of|the|for)$'}, 'OP': '*'},\n",
    "    {'POS': 'NOUN', 'IS_TITLE': '+', 'OP': '*'},\n",
    "    {'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-stereo",
   "metadata": {},
   "source": [
    "**Performances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "working-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_lexical = [\n",
    "    {'LOWER': {'REGEX': '^(star|starred|known|appeared|appearing|appearances|debute|album|recorded|play|played|role)$'}},\n",
    "    {'LOWER': {'REGEX': '^(in|for|on|for|as)$'}},\n",
    "    {'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "equipped-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_syntactic = [\n",
    "    {'LOWER': {'REGEX': '^(star|starred|known|appeared|appearing|appearances|debute|album|recorded|play|played|role)$'}},\n",
    "    {'POS': 'ADP', 'TEXT': {'REGEX': '^(in|of|the|for|as)$'}, 'OP': '+'},\n",
    "    {'OP': '+'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-headquarters",
   "metadata": {},
   "source": [
    "**Colleagues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "marked-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "colleagues_lexical = [\n",
    "    {'IS_TITLE': '+', 'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(starring|collaborating|working|worked|partnering|sharing.*screen.*space|appears.*opposite|opposite)$'}},\n",
    "    {'LOWER': {'REGEX': '^(with|alongside|to)$'}}, {'OP': '*'},\n",
    "    {'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "refined-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "colleagues_syntactic = [\n",
    "    {'OP': '*'},\n",
    "    {'LOWER': {'REGEX': '^(starring|collaborating|working|worked|partnering|sharing.*screen.*space|appears.*opposite|opposite|joined)$'}},\n",
    "    {'LOWER': {'REGEX': '^(with|alongside|to)$'}}, {'OP': '*'},\n",
    "    {'ENT_TYPE': 'PERSON'},\n",
    "    {'OP': '*'},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-fence",
   "metadata": {},
   "source": [
    "## Function to Match Extractors and Return Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "golden-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_matcher(doc, pattern):\n",
    "    output = []\n",
    "    \n",
    "    # Instantiate the spaCy Matcher\n",
    "    matcher = Matcher(nlp.vocab) \n",
    "    matcher.add(\"matching\", None, pattern)\n",
    "    \n",
    "    # Iterate throgh sentences\n",
    "    for sent in doc.sents:\n",
    "\n",
    "        # Match sentences to the pattern\n",
    "        matches = matcher(nlp(sent.text)) \n",
    "        \n",
    "        # If any matches were found, extract them and append to the list of matches for that pair of (document x pattern)\n",
    "        if len(matches)>0:\n",
    "            span = sent[matches[-1][1]:matches[-1][2]] \n",
    "            output.append(span.text)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-quality",
   "metadata": {},
   "source": [
    "## Data Cleaning and Filtering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "monthly-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthplace_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = \" \".join(list_with_text)        \n",
    "        list_with_text = joined.rsplit(\"as\")\n",
    "        return [[' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", list_with_text[i])) \n",
    "                 for i in range(len(list_with_text))][0]\n",
    "                .strip(r'^January$|^February$|^March$|^April$|^May$|^June$|^July$|^August$|^September$|^October$|^November$|^December$|^USA$')][0]\n",
    "    else:\n",
    "        return \"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "northern-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def education_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = \",\".join(list_with_text)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(\",| and \", joined)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Clear additional frequent nuisances\n",
    "        output1 = [re.sub(r'^She$|^He$|^January$|^February$|^March$|^April$|^May$|^June$|^July$|^August$|^September$|^October$|^November$|^December$', '', titlecases[i]) for i in range(len(titlecases))]\n",
    "        # Remove any empty strings generated\n",
    "        output2 = [x for x in output1 if x.strip()]\n",
    "        return output2\n",
    "    else:\n",
    "        return list_with_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dependent-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parents_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = \",\".join(list_with_text)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(\",| and \", joined)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Clear additional frequent nuisances\n",
    "        output1 = [re.sub(r'^She$|^He$|^January$|^February$|^March$|^April$|^May$|^June$|^July$|^August$|^September$|^October$|^November$|^December$', '', titlecases[i]) for i in range(len(titlecases))]\n",
    "        # Remove any empty strings generated\n",
    "        output2 = [x for x in output1 if x.strip()]\n",
    "        return output2\n",
    "    else:\n",
    "        return list_with_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "representative-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def awards_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = \",\".join(list_with_text)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(\",| and | for \", joined)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Remove any empty strings generated\n",
    "        output1 = [x for x in titlecases if x.strip()]\n",
    "        return output1\n",
    "    else:\n",
    "        return list_with_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "criminal-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Convert to a single single\n",
    "        joined = \",\".join(list_with_text)\n",
    "        # Remove \"known for\"\n",
    "        clean_1 = re.sub(\"known for\", \"\", joined)\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(\",| and \", clean_1)\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Remove any empty strings generated\n",
    "        output1 = [x for x in titlecases if x.strip()]\n",
    "        return output1\n",
    "    else:\n",
    "        return list_with_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "perceived-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colleagues_filter(list_with_text):\n",
    "    if len(list_with_text) > 0:\n",
    "        # Remove double quotations \" around some movie names\n",
    "        unquoted = [x.strip(\"'\") for x in list_with_text]\n",
    "        # Split the text considering punctions and other demarkings\n",
    "        text_splits = re.split(\",| and | in\", unquoted[0])\n",
    "        # Keep only Titlecasewords and their punctuations\n",
    "        titlecases = [' '.join(re.findall(r\"\\b(?:[A-Z][^\\s]*\\s?)+\", [text_splits[i] for i in range(len(text_splits))][x])) for x in range(len(text_splits))]\n",
    "        # Remove any empty strings generated\n",
    "        output1 = [x for x in titlecases if x.strip()]\n",
    "        return output1\n",
    "    else:\n",
    "        return list_with_text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-signal",
   "metadata": {},
   "source": [
    "## Run spaCy Extration on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "better-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"Matheus_Schmitz_hw02_bios.csv\"\n",
    "output_jl = \"spacy_extractions.jl\"\n",
    "extractor_number = 1 # 0 for lexical, 1 for syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "specified-champion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/982 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying syntactic extractor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 982/982 [26:03<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# Lexical\n",
    "if extractor_number == 0:\n",
    "    print('Applying lexical extractor.')\n",
    "    # Open the jsonlines to be written\n",
    "    with open(output_jl, 'w') as hw2_lexical:\n",
    "        # Iterate through the csv with bios\n",
    "        for url, bio in tqdm(csv.reader(open(input_csv, encoding='utf-8')), total=sum(1 for row in csv.reader(open(input_csv, encoding='utf-8')))):\n",
    "\n",
    "            # Create dict to store the outputs of each row\n",
    "            output = {}\n",
    "\n",
    "            # Populate the outputs with spaCy's Matcher\n",
    "            output[\"url\"] = url\n",
    "            output[\"birthplace\"] = spacy_matcher(nlp(bio), birthplace_lexical)\n",
    "            output[\"education\"] = spacy_matcher(nlp(bio), education_lexical)\n",
    "            output[\"parents\"] = spacy_matcher(nlp(bio), parents_lexical)\n",
    "            output[\"awards\"] = spacy_matcher(nlp(bio), awards_lexical)        \n",
    "            output[\"performances\"] = spacy_matcher(nlp(bio), performances_lexical)\n",
    "            output[\"colleagues\"] = spacy_matcher(nlp(bio), colleagues_lexical)\n",
    "\n",
    "            # Keep only nouns for certain outputs\n",
    "            output[\"birthplace\"] = birthplace_filter(output[\"birthplace\"])\n",
    "            output[\"education\"] = education_filter(output[\"education\"])\n",
    "            output[\"parents\"] = parents_filter(output[\"parents\"])\n",
    "            output[\"awards\"] = awards_filter(output[\"awards\"])\n",
    "            output[\"performances\"] = performances_filter(output[\"performances\"])\n",
    "            output[\"colleagues\"] = colleagues_filter(output[\"colleagues\"])\n",
    "\n",
    "            # Write a csv row as a line in jsonlines \n",
    "            json.dump(output, hw2_lexical)\n",
    "            hw2_lexical.write(\"\\n\")\n",
    "\n",
    "        # Close the jsonlines file\n",
    "        hw2_lexical.close()\n",
    "\n",
    "# Syntactic       \n",
    "elif extractor_number == 1:\n",
    "    print('Applying syntactic extractor.')\n",
    "    # Open the jsonlines to be written\n",
    "    with open(output_jl, 'w') as hw2_syntactic:\n",
    "        # Iterate through the csv with bios   \n",
    "        for url, bio in tqdm(csv.reader(open(input_csv, encoding='utf-8')), total=sum(1 for row in csv.reader(open(input_csv, encoding='utf-8')))):\n",
    "\n",
    "            # Create dict to store the outputs of each row\n",
    "            output = {}\n",
    "\n",
    "            # Populate the outputs with spaCy's Matcher\n",
    "            output[\"url\"] = url\n",
    "            output[\"birthplace\"] = spacy_matcher(nlp(bio), birthplace_syntactic)\n",
    "            output[\"education\"] = spacy_matcher(nlp(bio), education_syntactic)\n",
    "            output[\"parents\"] = spacy_matcher(nlp(bio), parents_syntactic)\n",
    "            output[\"awards\"] = spacy_matcher(nlp(bio), awards_syntactic)        \n",
    "            output[\"performances\"] = spacy_matcher(nlp(bio), performances_syntactic)\n",
    "            output[\"colleagues\"] = spacy_matcher(nlp(bio), colleagues_syntactic)\n",
    "\n",
    "            # Keep only nouns for certain outputs\n",
    "            output[\"birthplace\"] = birthplace_filter(output[\"birthplace\"])\n",
    "            output[\"education\"] = education_filter(output[\"education\"])\n",
    "            output[\"parents\"] = parents_filter(output[\"parents\"])\n",
    "            output[\"awards\"] = awards_filter(output[\"awards\"])\n",
    "            output[\"performances\"] = performances_filter(output[\"performances\"])\n",
    "            output[\"colleagues\"] = colleagues_filter(output[\"colleagues\"])\n",
    "\n",
    "            # Write a csv row as a line in jsonlines \n",
    "            json.dump(output, hw2_syntactic)\n",
    "            hw2_syntactic.write(\"\\n\")\n",
    "\n",
    "        # Close the jsonlines file\n",
    "        hw2_syntactic.close()\n",
    "\n",
    "# Error    \n",
    "else:\n",
    "    raise ValueError('Input either 0 for lexical patterns or 1 for syntactic patterns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-silver",
   "metadata": {},
   "source": [
    "## Visualize the Extracted Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "elder-pocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extractions_df.shape: (982, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>education</th>\n",
       "      <th>parents</th>\n",
       "      <th>awards</th>\n",
       "      <th>performances</th>\n",
       "      <th>colleagues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.imdb.com/name/nm2604107/bio</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Schiri  Abseits , Der Fahnder , Streit  Drei ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.imdb.com/name/nm0001873/bio</td>\n",
       "      <td>Fresno, California,</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Searching  Bobby Fischer , Schindler's List ,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.imdb.com/name/nm12027010/bio</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[M , Dziewczyny  Dubaju , Barwy ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.imdb.com/name/nm11017764/bio</td>\n",
       "      <td>Morbach,</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.imdb.com/name/nm0014294/bio</td>\n",
       "      <td>Salzburg, Austria.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Anatomy , Rote Rosen , Brandnacht ]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        url             birthplace education  \\\n",
       "0   https://www.imdb.com/name/nm2604107/bio                               []   \n",
       "1   https://www.imdb.com/name/nm0001873/bio    Fresno, California,        []   \n",
       "2  https://www.imdb.com/name/nm12027010/bio                               []   \n",
       "3  https://www.imdb.com/name/nm11017764/bio               Morbach,        []   \n",
       "4   https://www.imdb.com/name/nm0014294/bio     Salzburg, Austria.        []   \n",
       "\n",
       "  parents awards                                       performances colleagues  \n",
       "0      []     []    [Schiri  Abseits , Der Fahnder , Streit  Drei ]         []  \n",
       "1      []     []  [Searching  Bobby Fischer , Schindler's List ,...         []  \n",
       "2      []     []                  [M , Dziewczyny  Dubaju , Barwy ]         []  \n",
       "3      []     []                                                 []         []  \n",
       "4      []     []               [Anatomy , Rote Rosen , Brandnacht ]         []  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractions_df = pd.read_json(output_jl, lines=True)\n",
    "print(f'extractions_df.shape: {extractions_df.shape}')\n",
    "extractions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-junior",
   "metadata": {},
   "source": [
    "**View all extractions for one sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sonic-absence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.imdb.com/name/nm0000432/bio',\n",
       " 'birthplace': ' Bernardino, California,',\n",
       " 'education': ['University  Illinois'],\n",
       " 'parents': ['San Bernardino',\n",
       "  'California',\n",
       "  'Anna Lyda Elizabeth  Gray)',\n",
       "  'Eugene Ezra Hackman'],\n",
       " 'awards': ['Oscar',\n",
       "  'Detective Jimmy  Popeye\" Doyle  The French Connection ',\n",
       "  'Oscar',\n",
       "  'Golden Globe'],\n",
       " 'performances': ['Norman  Lilith ',\n",
       "  'Warren Beatty.',\n",
       "  \"Little Bill Daggett  Clint Eastwood's Unforgiven \",\n",
       "  'Hackman ',\n",
       "  'Geronimo:'],\n",
       " 'colleagues': []}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractions_df.iloc[180].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-remove",
   "metadata": {},
   "source": [
    "# End\n",
    "Matheus Schmitz\n",
    "<br><a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a></br>\n",
    "<br><a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
